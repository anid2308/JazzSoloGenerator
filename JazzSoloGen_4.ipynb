{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt at \"vibe-coding\" this project"
      ],
      "metadata": {
        "id": "-nR39Siis1mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CysR69R7sNed",
        "outputId": "e5a5518d-b6f4-49b1-85a8-dfc0a47ee8b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25htorch: 2.9.0+cu126\n",
            "cuda available? True\n",
            "device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0fe9cf26-5924-4b67-941e-3974c338d52a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0fe9cf26-5924-4b67-941e-3974c338d52a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 10 Jazz Etudes Midi.zip to 10 Jazz Etudes Midi.zip\n",
            "Found MIDI files: 10\n",
            "Example paths: ['/content/midis/10 Jazz Etudes Midi/AnyConv.com__Etude #1 - No Met- Slow.midi', '/content/midis/10 Jazz Etudes Midi/AnyConv.com__Etude #10 - No Met- Slow.midi', '/content/midis/10 Jazz Etudes Midi/AnyConv.com__Etude #2 - No Met- Slow.midi']\n",
            "\n",
            "name | tracks | drums | notes | same_start | max_poly\n",
            "AnyConv.com__Etude #1 - No M |      1 | False |  1388 |          9 |       12\n",
            "AnyConv.com__Etude #10 - No  |      1 | False |  1246 |          7 |       11\n",
            "AnyConv.com__Etude #2 - No M |      1 | False |  1649 |          8 |       13\n",
            "AnyConv.com__Etude #3- No Me |      1 | False |  1433 |          8 |       14\n",
            "AnyConv.com__Etude #4- No Me |      1 | False |  1312 |          8 |       12\n",
            "AnyConv.com__Etude #5- No Me |      1 | False |  1157 |          7 |       13\n",
            "AnyConv.com__Etude #6- No Me |      1 | False |  1284 |          8 |       12\n",
            "AnyConv.com__Etude #7- No Me |      1 | False |  1404 |          7 |       13\n",
            "AnyConv.com__Etude #8- No Me |      1 | False |  1386 |          8 |       14\n",
            "AnyConv.com__Etude #9- No Me |      1 | False |  1153 |          8 |       12\n",
            "\n",
            "Token stats:\n",
            "Files: 10\n",
            "Total tokens: 12967\n",
            "Example first 60 tokens: ['BAR', 'POS_0', 'NOTE_98', 'DUR_1', 'POS_1', 'NOTE_87', 'DUR_1', 'REST_10', 'POS_0', 'NOTE_99', 'DUR_1', 'POS_1', 'NOTE_90', 'DUR_1', 'REST_11', 'POS_1', 'NOTE_98', 'DUR_1', 'REST_11', 'POS_1', 'NOTE_98', 'DUR_1', 'REST_11', 'POS_1', 'NOTE_72', 'DUR_1', 'POS_2', 'NOTE_60', 'DUR_11', 'REST_1', 'POS_2', 'NOTE_86', 'DUR_1', 'POS_3', 'NOTE_42', 'DUR_1', 'REST_5', 'POS_9', 'NOTE_71', 'DUR_1', 'POS_10', 'NOTE_59', 'DUR_4', 'POS_2', 'NOTE_64', 'DUR_7', 'POS_9', 'NOTE_75', 'DUR_1', 'POS_10', 'NOTE_63', 'DUR_2', 'POS_0', 'NOTE_42', 'DUR_1', 'REST_1', 'POS_2', 'NOTE_86', 'DUR_1', 'POS_3']\n",
            "\n",
            "Vocab size: 240\n",
            "\n",
            "Train ids: [2, 8, 5, 6, 9, 4, 0, 1]\n",
            "Val ids: [3, 7]\n",
            "Train files: ['AnyConv.com__Etude #2 - No Met- Slow.midi', 'AnyConv.com__Etude #8- No Met- Slow.midi', 'AnyConv.com__Etude #5- No Met- Slow.midi', 'AnyConv.com__Etude #6- No Met- Slow.midi', 'AnyConv.com__Etude #9- No Met- Slow.midi', 'AnyConv.com__Etude #4- No Met- Slow.midi', 'AnyConv.com__Etude #1 - No Met- Slow.midi', 'AnyConv.com__Etude #10 - No Met- Slow.midi']\n",
            "Val files: ['AnyConv.com__Etude #3- No Met- Slow.midi', 'AnyConv.com__Etude #7- No Met- Slow.midi']\n",
            "Built 133 windows from 8 files. augment=True\n",
            "Built 34 windows from 2 files. augment=False\n",
            "\n",
            "Model on: cuda:0\n",
            "epoch 001 | train_loss=5.3419 | val_loss=4.6517\n",
            "  saved best checkpoint\n",
            "epoch 002 | train_loss=4.5853 | val_loss=4.3526\n",
            "  saved best checkpoint\n",
            "epoch 003 | train_loss=4.3388 | val_loss=4.2238\n",
            "  saved best checkpoint\n",
            "epoch 004 | train_loss=4.2063 | val_loss=4.1226\n",
            "  saved best checkpoint\n",
            "epoch 005 | train_loss=4.1109 | val_loss=4.0078\n",
            "  saved best checkpoint\n",
            "epoch 006 | train_loss=4.0076 | val_loss=3.8900\n",
            "  saved best checkpoint\n",
            "epoch 007 | train_loss=3.8994 | val_loss=3.7916\n",
            "  saved best checkpoint\n",
            "epoch 008 | train_loss=3.7988 | val_loss=3.6944\n",
            "  saved best checkpoint\n",
            "epoch 009 | train_loss=3.6897 | val_loss=3.5848\n",
            "  saved best checkpoint\n",
            "epoch 010 | train_loss=3.5822 | val_loss=3.4783\n",
            "  saved best checkpoint\n",
            "epoch 011 | train_loss=3.4702 | val_loss=3.3721\n",
            "  saved best checkpoint\n",
            "epoch 012 | train_loss=3.3652 | val_loss=3.2702\n",
            "  saved best checkpoint\n",
            "epoch 013 | train_loss=3.2786 | val_loss=3.1911\n",
            "  saved best checkpoint\n",
            "epoch 014 | train_loss=3.2108 | val_loss=3.1287\n",
            "  saved best checkpoint\n",
            "epoch 015 | train_loss=3.1508 | val_loss=3.0825\n",
            "  saved best checkpoint\n",
            "epoch 016 | train_loss=3.0998 | val_loss=3.0506\n",
            "  saved best checkpoint\n",
            "epoch 017 | train_loss=3.0706 | val_loss=3.0177\n",
            "  saved best checkpoint\n",
            "epoch 018 | train_loss=3.0369 | val_loss=2.9927\n",
            "  saved best checkpoint\n",
            "epoch 019 | train_loss=3.0096 | val_loss=2.9716\n",
            "  saved best checkpoint\n",
            "epoch 020 | train_loss=2.9862 | val_loss=2.9570\n",
            "  saved best checkpoint\n",
            "epoch 021 | train_loss=2.9639 | val_loss=2.9435\n",
            "  saved best checkpoint\n",
            "epoch 022 | train_loss=2.9435 | val_loss=2.9246\n",
            "  saved best checkpoint\n",
            "epoch 023 | train_loss=2.9227 | val_loss=2.9106\n",
            "  saved best checkpoint\n",
            "epoch 024 | train_loss=2.9066 | val_loss=2.9014\n",
            "  saved best checkpoint\n",
            "epoch 025 | train_loss=2.8889 | val_loss=2.8900\n",
            "  saved best checkpoint\n",
            "epoch 026 | train_loss=2.8784 | val_loss=2.8802\n",
            "  saved best checkpoint\n",
            "epoch 027 | train_loss=2.8637 | val_loss=2.8771\n",
            "  saved best checkpoint\n",
            "epoch 028 | train_loss=2.8488 | val_loss=2.8698\n",
            "  saved best checkpoint\n",
            "epoch 029 | train_loss=2.8440 | val_loss=2.8642\n",
            "  saved best checkpoint\n",
            "epoch 030 | train_loss=2.8275 | val_loss=2.8591\n",
            "  saved best checkpoint\n",
            "epoch 031 | train_loss=2.8205 | val_loss=2.8496\n",
            "  saved best checkpoint\n",
            "epoch 032 | train_loss=2.8040 | val_loss=2.8381\n",
            "  saved best checkpoint\n",
            "epoch 033 | train_loss=2.7981 | val_loss=2.8287\n",
            "  saved best checkpoint\n",
            "epoch 034 | train_loss=2.7869 | val_loss=2.8186\n",
            "  saved best checkpoint\n",
            "epoch 035 | train_loss=2.7723 | val_loss=2.8130\n",
            "  saved best checkpoint\n",
            "epoch 036 | train_loss=2.7611 | val_loss=2.8087\n",
            "  saved best checkpoint\n",
            "epoch 037 | train_loss=2.7504 | val_loss=2.8039\n",
            "  saved best checkpoint\n",
            "epoch 038 | train_loss=2.7371 | val_loss=2.7973\n",
            "  saved best checkpoint\n",
            "epoch 039 | train_loss=2.7263 | val_loss=2.7892\n",
            "  saved best checkpoint\n",
            "epoch 040 | train_loss=2.7111 | val_loss=2.7811\n",
            "  saved best checkpoint\n",
            "epoch 041 | train_loss=2.7013 | val_loss=2.7763\n",
            "  saved best checkpoint\n",
            "epoch 042 | train_loss=2.6914 | val_loss=2.7726\n",
            "  saved best checkpoint\n",
            "epoch 043 | train_loss=2.6753 | val_loss=2.7664\n",
            "  saved best checkpoint\n",
            "epoch 044 | train_loss=2.6659 | val_loss=2.7729\n",
            "epoch 045 | train_loss=2.6602 | val_loss=2.7584\n",
            "  saved best checkpoint\n",
            "epoch 046 | train_loss=2.6422 | val_loss=2.7449\n",
            "  saved best checkpoint\n",
            "epoch 047 | train_loss=2.6296 | val_loss=2.7477\n",
            "epoch 048 | train_loss=2.6273 | val_loss=2.7370\n",
            "  saved best checkpoint\n",
            "epoch 049 | train_loss=2.6090 | val_loss=2.7207\n",
            "  saved best checkpoint\n",
            "epoch 050 | train_loss=2.5952 | val_loss=2.7248\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 0) Setup (install + imports + GPU check)\n",
        "# =========================================\n",
        "!pip -q install miditoolkit tqdm\n",
        "\n",
        "import os, glob, zipfile, random, math\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import miditoolkit\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available?\", torch.cuda.is_available())\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "# =========================================\n",
        "# 1) Upload ZIP of MIDI files -> midi_paths\n",
        "# =========================================\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload your zip of 10 MIDIs (no-metronome)\n",
        "\n",
        "zip_name = next(iter(uploaded.keys()))\n",
        "DATA_DIR = \"/content/midis\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_name, \"r\") as z:\n",
        "    z.extractall(DATA_DIR)\n",
        "\n",
        "midi_paths = sorted(glob.glob(os.path.join(DATA_DIR, \"**/*.mid\"), recursive=True)) + \\\n",
        "            sorted(glob.glob(os.path.join(DATA_DIR, \"**/*.midi\"), recursive=True))\n",
        "\n",
        "print(\"Found MIDI files:\", len(midi_paths))\n",
        "print(\"Example paths:\", midi_paths[:3])\n",
        "\n",
        "# =========================================\n",
        "# 2) Quick scan: tracks/drums/polyphony stats\n",
        "# =========================================\n",
        "def max_distinct_pitches_same_start(m):\n",
        "    inst = m.instruments[0]\n",
        "    by = defaultdict(set)\n",
        "    for n in inst.notes:\n",
        "        by[n.start].add(n.pitch)\n",
        "    return max((len(v) for v in by.values()), default=0)\n",
        "\n",
        "def max_simultaneous_notes(notes):\n",
        "    events = []\n",
        "    for n in notes:\n",
        "        events.append((n.start, 1))\n",
        "        events.append((n.end, -1))\n",
        "    events.sort()\n",
        "    cur = 0\n",
        "    mx = 0\n",
        "    for _, d in events:\n",
        "        cur += d\n",
        "        mx = max(mx, cur)\n",
        "    return mx\n",
        "\n",
        "print(\"\\nname | tracks | drums | notes | same_start | max_poly\")\n",
        "for p in midi_paths:\n",
        "    m = miditoolkit.MidiFile(p)\n",
        "    inst = m.instruments[0]\n",
        "    print(f\"{os.path.basename(p)[:28]:28} | {len(m.instruments):6d} | {str(any(i.is_drum for i in m.instruments)):5s} \"\n",
        "          f\"| {len(inst.notes):5d} | {max_distinct_pitches_same_start(m):10d} | {max_simultaneous_notes(inst.notes):8d}\")\n",
        "\n",
        "# =========================================\n",
        "# 3) Clean to monophonic + tokenize (BAR + POS + REST + NOTE + DUR)\n",
        "# =========================================\n",
        "STEPS_PER_BEAT = 12\n",
        "BAR_STEPS = 4 * STEPS_PER_BEAT  # assume 4/4\n",
        "MAX_DUR = 48\n",
        "MAX_REST = 48\n",
        "\n",
        "def ticks_to_steps(ticks, tpb, spb=STEPS_PER_BEAT):\n",
        "    return int(round((ticks / tpb) * spb))\n",
        "\n",
        "def clean_to_monophonic(notes, tpb, spb=STEPS_PER_BEAT):\n",
        "    \"\"\"\n",
        "    Convert messy/polyphonic notes into a single line by:\n",
        "    1) quantize to steps\n",
        "    2) keep 1 note per onset step (longest dur, tie -> highest pitch)\n",
        "    3) trim overlaps\n",
        "    Returns list of (start_step, end_step, pitch)\n",
        "    \"\"\"\n",
        "    if not notes:\n",
        "        return []\n",
        "\n",
        "    q = []\n",
        "    for n in notes:\n",
        "        s = ticks_to_steps(n.start, tpb, spb)\n",
        "        e = ticks_to_steps(n.end,   tpb, spb)\n",
        "        if e <= s:\n",
        "            e = s + 1\n",
        "        q.append((s, e, int(n.pitch)))\n",
        "\n",
        "    by_s = defaultdict(list)\n",
        "    for s, e, p in q:\n",
        "        by_s[s].append((s, e, p))\n",
        "\n",
        "    kept = [max(g, key=lambda x: ((x[1]-x[0]), x[2])) for g in by_s.values()]\n",
        "    kept.sort(key=lambda x: (x[0], x[2]))\n",
        "\n",
        "    mono = []\n",
        "    for s, e, p in kept:\n",
        "        if mono and s < mono[-1][1]:\n",
        "            ps, pe, pp = mono[-1]\n",
        "            mono[-1] = (ps, s, pp)\n",
        "            if mono[-1][1] <= mono[-1][0]:\n",
        "                mono.pop()\n",
        "        mono.append((s, e, p))\n",
        "\n",
        "    mono = [x for x in mono if x[1] > x[0]]\n",
        "    return mono\n",
        "\n",
        "def tokens_from_mono_with_pos(mono, max_rest=MAX_REST, max_dur=MAX_DUR, add_bar=True):\n",
        "    def emit(out, prefix, v, cap):\n",
        "        while v > cap:\n",
        "            out.append(f\"{prefix}_{cap}\")\n",
        "            v -= cap\n",
        "        out.append(f\"{prefix}_{v}\")\n",
        "\n",
        "    toks = []\n",
        "    cur = 0\n",
        "    for s, e, p in mono:\n",
        "        # mark bar at note start if exactly on boundary\n",
        "        if add_bar and (s % BAR_STEPS == 0):\n",
        "            toks.append(\"BAR\")\n",
        "\n",
        "        # advance time with REST chunks (and insert BAR when crossing boundaries)\n",
        "        while cur < s:\n",
        "            if add_bar and (cur % BAR_STEPS == 0):\n",
        "                toks.append(\"BAR\")\n",
        "            step = min(s - cur, max_rest)\n",
        "            toks.append(f\"REST_{step}\")\n",
        "            cur += step\n",
        "\n",
        "        # beat-position anchor\n",
        "        toks.append(f\"POS_{s % STEPS_PER_BEAT}\")\n",
        "\n",
        "        toks.append(f\"NOTE_{p}\")\n",
        "        dur = max(1, e - s)\n",
        "        emit(toks, \"DUR\", dur, max_dur)\n",
        "        cur = e\n",
        "\n",
        "    return toks\n",
        "\n",
        "# Build per-file tokens\n",
        "per_file_tokens = []\n",
        "per_file_mono = []\n",
        "for p in midi_paths:\n",
        "    m = miditoolkit.MidiFile(p)\n",
        "    inst = m.instruments[0]\n",
        "    mono = clean_to_monophonic(inst.notes, m.ticks_per_beat, spb=STEPS_PER_BEAT)\n",
        "    toks = tokens_from_mono_with_pos(mono, add_bar=True)\n",
        "    per_file_mono.append(mono)\n",
        "    per_file_tokens.append(toks)\n",
        "\n",
        "print(\"\\nToken stats:\")\n",
        "print(\"Files:\", len(per_file_tokens))\n",
        "print(\"Total tokens:\", sum(len(t) for t in per_file_tokens))\n",
        "print(\"Example first 60 tokens:\", per_file_tokens[0][:60])\n",
        "\n",
        "# =========================================\n",
        "# 4) Build vocab + encode\n",
        "# =========================================\n",
        "PAD, BOS, EOS = \"<PAD>\", \"<BOS>\", \"<EOS>\"\n",
        "\n",
        "vocab = [PAD, BOS, EOS, \"BAR\"]\n",
        "vocab += [f\"POS_{i}\" for i in range(STEPS_PER_BEAT)]\n",
        "vocab += [f\"NOTE_{p}\" for p in range(128)]\n",
        "vocab += [f\"DUR_{d}\" for d in range(1, MAX_DUR+1)]\n",
        "vocab += [f\"REST_{r}\" for r in range(1, MAX_REST+1)]\n",
        "\n",
        "stoi = {t:i for i,t in enumerate(vocab)}\n",
        "itos = {i:t for t,i in stoi.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(\"\\nVocab size:\", vocab_size)\n",
        "\n",
        "def encode(tokens): return [stoi[t] for t in tokens]\n",
        "def decode(ids): return [itos[i] for i in ids]\n",
        "\n",
        "encoded_files = [encode(toks) for toks in per_file_tokens]\n",
        "\n",
        "# =========================================\n",
        "# 5) Train/Val split by FILE (no leakage)\n",
        "# =========================================\n",
        "rng = random.Random(42)\n",
        "idxs = list(range(len(encoded_files)))\n",
        "rng.shuffle(idxs)\n",
        "\n",
        "val_n = max(1, len(idxs)//5)  # ~20% val (2 files if you have 10)\n",
        "val_ids = sorted(idxs[:val_n])\n",
        "train_ids = [i for i in idxs if i not in set(val_ids)]\n",
        "\n",
        "print(\"\\nTrain ids:\", train_ids)\n",
        "print(\"Val ids:\", val_ids)\n",
        "print(\"Train files:\", [os.path.basename(midi_paths[i]) for i in train_ids])\n",
        "print(\"Val files:\", [os.path.basename(midi_paths[i]) for i in val_ids])\n",
        "\n",
        "# =========================================\n",
        "# 6) Window Dataset + (optional) transpose augmentation\n",
        "# =========================================\n",
        "block_size = 256\n",
        "stride = 64\n",
        "\n",
        "# Use observed pitch range if you want safer transposition\n",
        "NOTE_MIN = 39\n",
        "NOTE_MAX = 100\n",
        "\n",
        "NOTE0 = stoi[\"NOTE_0\"]\n",
        "NOTE127 = stoi[\"NOTE_127\"]\n",
        "\n",
        "def is_note_id(tid): return NOTE0 <= tid <= NOTE127\n",
        "def note_pitch_from_id(tid): return tid - NOTE0\n",
        "def note_id_from_pitch(p): return NOTE0 + p\n",
        "\n",
        "class JazzWindowDataset(Dataset):\n",
        "    def __init__(self, encoded_files, file_ids, block_size=256, stride=64, augment=False):\n",
        "        self.encoded_files = encoded_files\n",
        "        self.file_ids = file_ids\n",
        "        self.block_size = block_size\n",
        "        self.augment = augment\n",
        "        self.windows = []\n",
        "\n",
        "        for fid in file_ids:\n",
        "            seq = encoded_files[fid]\n",
        "            for s in range(0, len(seq) - block_size, stride):\n",
        "                self.windows.append((fid, s))\n",
        "\n",
        "        print(f\"Built {len(self.windows)} windows from {len(file_ids)} files. augment={augment}\")\n",
        "\n",
        "    def __len__(self): return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fid, s = self.windows[idx]\n",
        "        seq = self.encoded_files[fid]\n",
        "        chunk = seq[s:s+self.block_size]  # length block_size\n",
        "\n",
        "        # transpose augmentation: only affect NOTE tokens\n",
        "        if self.augment:\n",
        "            pitches = [note_pitch_from_id(t) for t in chunk if is_note_id(t)]\n",
        "            if pitches:\n",
        "                lo, hi = min(pitches), max(pitches)\n",
        "                down = NOTE_MIN - lo\n",
        "                up = NOTE_MAX - hi\n",
        "                if down <= up:\n",
        "                    shift = random.randint(down, up)\n",
        "                    if shift != 0:\n",
        "                        chunk = [\n",
        "                            (note_id_from_pitch(note_pitch_from_id(t) + shift) if is_note_id(t) else t)\n",
        "                            for t in chunk\n",
        "                        ]\n",
        "\n",
        "        x = [stoi[BOS]] + chunk          # len = block_size+1\n",
        "        y = chunk + [stoi[EOS]]          # len = block_size+1\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "train_ds = JazzWindowDataset(encoded_files, train_ids, block_size=block_size, stride=stride, augment=True)\n",
        "val_ds   = JazzWindowDataset(encoded_files, val_ids,   block_size=block_size, stride=stride, augment=False)\n",
        "\n",
        "batch_size = 64 if device == \"cuda\" else 16\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "# =========================================\n",
        "# 7) GPT-style Transformer (decoder-only via causal mask)\n",
        "# =========================================\n",
        "class GPTMini(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=6, dropout=0.15, max_len=block_size+2):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=4*d_model,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=False,  # avoids that nested-tensor warning sometimes\n",
        "            activation=\"gelu\",\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        if T > self.max_len:\n",
        "            raise ValueError(f\"T={T} > max_len={self.max_len}\")\n",
        "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        causal = torch.triu(torch.ones(T, T, device=idx.device), diagonal=1).bool()\n",
        "        x = self.encoder(x, mask=causal)\n",
        "\n",
        "        x = self.ln(x)\n",
        "        return self.head(x)  # (B,T,V)\n",
        "\n",
        "model = GPTMini(vocab_size).to(device)\n",
        "print(\"\\nModel on:\", next(model.parameters()).device)\n",
        "\n",
        "# =========================================\n",
        "# 8) Train loop + eval + checkpoint\n",
        "# =========================================\n",
        "def run_eval(model, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss(ignore_index=stoi[PAD])\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = ce(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            total_tokens += y.numel()\n",
        "    return total_loss / max(1, total_tokens)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "ce = nn.CrossEntropyLoss(ignore_index=stoi[PAD])\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "os.makedirs(\"/content/ckpt\", exist_ok=True)\n",
        "\n",
        "EPOCHS = 50\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running = []\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running.append(loss.item())\n",
        "\n",
        "    val_loss = run_eval(model, val_loader)\n",
        "    train_loss = float(np.mean(running)) if running else float(\"nan\")\n",
        "    print(f\"epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        torch.save({\n",
        "            \"model\": model.state_dict(),\n",
        "            \"opt\": opt.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"stoi\": stoi,\n",
        "            \"itos\": itos,\n",
        "            \"config\": {\"STEPS_PER_BEAT\": STEPS_PER_BEAT, \"block_size\": block_size}\n",
        "        }, \"/content/ckpt/best.pt\")\n",
        "        print(\"  saved best checkpoint\")\n",
        "\n",
        "# =========================================\n",
        "# 9) Compact generation (grammar masks) + MIDI writer\n",
        "# =========================================\n",
        "def build_type_masks(vocab, stoi, device):\n",
        "    def is_note(tok): return tok.startswith(\"NOTE_\")\n",
        "    def is_dur(tok):  return tok.startswith(\"DUR_\")\n",
        "    def is_rest(tok): return tok.startswith(\"REST_\")\n",
        "    def is_pos(tok):  return tok.startswith(\"POS_\")\n",
        "\n",
        "    banned = torch.zeros(len(vocab), dtype=torch.bool, device=device)\n",
        "    banned[stoi[PAD]] = True\n",
        "    banned[stoi[BOS]] = True\n",
        "\n",
        "    masks = {}\n",
        "    masks[\"NOTE\"] = torch.tensor([is_note(t) for t in vocab], device=device) & ~banned\n",
        "    masks[\"DUR\"]  = torch.tensor([is_dur(t)  for t in vocab], device=device) & ~banned\n",
        "    masks[\"REST\"] = torch.tensor([is_rest(t) for t in vocab], device=device) & ~banned\n",
        "    masks[\"POS\"]  = torch.tensor([is_pos(t)  for t in vocab], device=device) & ~banned\n",
        "\n",
        "    eos_mask = torch.zeros(len(vocab), dtype=torch.bool, device=device)\n",
        "    eos_mask[stoi[EOS]] = True\n",
        "    masks[\"EOS\"] = eos_mask & ~banned\n",
        "\n",
        "    bar_mask = torch.zeros(len(vocab), dtype=torch.bool, device=device)\n",
        "    bar_mask[stoi[\"BAR\"]] = True\n",
        "    masks[\"BAR\"] = bar_mask & ~banned\n",
        "\n",
        "    return masks\n",
        "\n",
        "masks = build_type_masks(vocab, stoi, device=device)\n",
        "\n",
        "def topk_sample(logits, k=20, temperature=0.9):\n",
        "    logits = logits / max(temperature, 1e-6)\n",
        "    if k is not None and k < logits.numel():\n",
        "        v, _ = torch.topk(logits, k)\n",
        "        logits = logits.masked_fill(logits < v[-1], -1e9)\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(probs, 1).item()\n",
        "\n",
        "def generate_tokens(model, max_new_tokens=900, temperature=0.9, top_k=20):\n",
        "    \"\"\"\n",
        "    Grammar (based on how we tokenized):\n",
        "      BOS -> BAR or POS\n",
        "      BAR -> REST or POS\n",
        "      REST -> REST or BAR or POS\n",
        "      POS -> NOTE\n",
        "      NOTE -> DUR\n",
        "      DUR -> REST or BAR or POS or EOS\n",
        "    \"\"\"\n",
        "    allowed_by_prev = {\n",
        "        \"BOS\":  (masks[\"BAR\"] | masks[\"POS\"]),\n",
        "        \"BAR\":  (masks[\"REST\"] | masks[\"POS\"]),\n",
        "        \"REST\": (masks[\"REST\"] | masks[\"BAR\"] | masks[\"POS\"]),\n",
        "        \"POS\":  masks[\"NOTE\"],\n",
        "        \"NOTE\": masks[\"DUR\"],\n",
        "        \"DUR\":  (masks[\"REST\"] | masks[\"BAR\"] | masks[\"POS\"] | masks[\"EOS\"]),\n",
        "    }\n",
        "\n",
        "    model.eval()\n",
        "    ids = [stoi[BOS]]\n",
        "    prev_type = \"BOS\"\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        x = torch.tensor(ids[-(block_size+1):], device=device).unsqueeze(0)\n",
        "        logits = model(x)[0, -1]  # (V,)\n",
        "\n",
        "        allowed = allowed_by_prev.get(prev_type, (masks[\"REST\"] | masks[\"POS\"] | masks[\"BAR\"] | masks[\"EOS\"]))\n",
        "        logits = logits.masked_fill(~allowed, -1e9)\n",
        "\n",
        "        nxt = topk_sample(logits, k=top_k, temperature=temperature)\n",
        "        ids.append(nxt)\n",
        "\n",
        "        tok = itos[nxt]\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        if tok == \"BAR\":\n",
        "            prev_type = \"BAR\"\n",
        "        else:\n",
        "            prev_type = tok.split(\"_\")[0]  # POS / NOTE / DUR / REST\n",
        "\n",
        "    return [itos[i] for i in ids]\n",
        "\n",
        "def tokens_to_midi(tokens, out_path=\"generated.mid\", tempo=140, steps_per_beat=STEPS_PER_BEAT):\n",
        "    tpb = 480\n",
        "    ticks_per_step = tpb // steps_per_beat\n",
        "\n",
        "    midi = miditoolkit.MidiFile(ticks_per_beat=tpb)\n",
        "    midi.tempo_changes = [miditoolkit.TempoChange(tempo, 0)]\n",
        "    inst = miditoolkit.Instrument(program=56, is_drum=False, name=\"Trumpet\")\n",
        "\n",
        "    t = 0\n",
        "    pending_pitch = None\n",
        "\n",
        "    for tok in tokens:\n",
        "        if tok in (PAD, BOS):\n",
        "            continue\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        if tok == \"BAR\" or tok.startswith(\"POS_\"):\n",
        "            continue\n",
        "\n",
        "        typ, val = tok.split(\"_\")\n",
        "        val = int(val)\n",
        "\n",
        "        if typ == \"REST\":\n",
        "            t += val\n",
        "            pending_pitch = None\n",
        "        elif typ == \"NOTE\":\n",
        "            pending_pitch = val\n",
        "        elif typ == \"DUR\" and pending_pitch is not None:\n",
        "            start = t * ticks_per_step\n",
        "            end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Best Checkpoint\n"
      ],
      "metadata": {
        "id": "HEEGq4BztkW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = torch.load(\"/content/ckpt/best.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"Loaded best checkpoint from epoch:\", ckpt[\"epoch\"], \"val_loss:\", ckpt[\"val_loss\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRv7g32jsaMb",
        "outputId": "fc512322-e39d-4a79-c2a8-2e5208b5ca21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best checkpoint from epoch: 49 val_loss: 2.720655679702759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_midi(tokens, out_path=\"generated.mid\", tempo=140, steps_per_beat=STEPS_PER_BEAT):\n",
        "    tpb = 480\n",
        "    ticks_per_step = tpb // steps_per_beat\n",
        "\n",
        "    midi = miditoolkit.MidiFile(ticks_per_beat=tpb)\n",
        "    midi.tempo_changes = [miditoolkit.TempoChange(tempo, 0)]\n",
        "    inst = miditoolkit.Instrument(program=56, is_drum=False, name=\"Trumpet\")\n",
        "\n",
        "    t = 0\n",
        "    pending_pitch = None\n",
        "\n",
        "    for tok in tokens:\n",
        "        if tok in (PAD, BOS):\n",
        "            continue\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        if tok == \"BAR\" or tok.startswith(\"POS_\"):\n",
        "            continue\n",
        "\n",
        "        typ, val = tok.split(\"_\")\n",
        "        val = int(val)\n",
        "\n",
        "        if typ == \"REST\":\n",
        "            t += val\n",
        "            pending_pitch = None\n",
        "        elif typ == \"NOTE\":\n",
        "            pending_pitch = val\n",
        "        elif typ == \"DUR\" and pending_pitch is not None:\n",
        "            start = t * ticks_per_step\n",
        "            end = (t + val) * ticks_per_step\n",
        "            inst.notes.append(miditoolkit.Note(velocity=90, pitch=pending_pitch, start=start, end=end))\n",
        "            t += val\n",
        "            pending_pitch = None\n",
        "\n",
        "    midi.instruments.append(inst)\n",
        "    midi.dump(out_path)\n",
        "    return out_path\n"
      ],
      "metadata": {
        "id": "wWo9v86Ttpxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings = [\n",
        "    (\"A\", 0.75, 10),\n",
        "    (\"B\", 0.85, 15),\n",
        "    (\"C\", 0.95, 25),\n",
        "]\n",
        "\n",
        "for tag, temp, k in settings:\n",
        "    gen = generate_tokens(model, max_new_tokens=1400, temperature=temp, top_k=k)\n",
        "    path = f\"gen_{tag}_t{temp}_k{k}.mid\"\n",
        "    tokens_to_midi(gen, out_path=path, tempo=140)\n",
        "    print(\"wrote\", path, \"| bars:\", sum(1 for t in gen if t==\"BAR\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKaQusrQt3ey",
        "outputId": "6dee7f37-d482-4a6c-d614-90986ff55d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrote gen_A_t0.75_k10.mid | bars: 0\n",
            "wrote gen_B_t0.85_k15.mid | bars: 1\n",
            "wrote gen_C_t0.95_k25.mid | bars: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9IFR0O8uDIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}