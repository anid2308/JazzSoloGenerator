{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nR39Siis1mS"
      },
      "source": [
        "## Attempt at \"vibe-coding\" this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CysR69R7sNed",
        "outputId": "e5a5518d-b6f4-49b1-85a8-dfc0a47ee8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.10.0\n",
            "cuda available? False\n",
            "device: cpu\n",
            "Running in local\n",
            "Found MIDI files: 7\n",
            "Example paths: ['/Users/jasoncharwin/Personal Code Projects/JazzSoloGenerator/data/raw/autumn_leaves_chet_baker.mid', '/Users/jasoncharwin/Personal Code Projects/JazzSoloGenerator/data/raw/autumn_leaves_wynton_marsalis.mid', '/Users/jasoncharwin/Personal Code Projects/JazzSoloGenerator/data/raw/have_you_met_miss_jones_chet_baker.mid']\n",
            "\n",
            "name | tracks | drums | notes | same_start | max_poly\n",
            "autumn_leaves_chet_baker.mid |      2 | False |   144 |          1 |        1\n",
            "autumn_leaves_wynton_marsali |      2 | False |   107 |          1 |        1\n",
            "have_you_met_miss_jones_chet |      2 | False |   473 |          1 |        1\n",
            "it_could_happen_to_you_miles |      2 | False |   250 |          1 |        1\n",
            "polkadots_and_moonbeams_chet |      2 | False |   181 |          1 |        1\n",
            "take_the_a_train_ryan_kisor. |      2 | False |   125 |          1 |        1\n",
            "there_will_never_be_another_ |      2 | False |   139 |          1 |        1\n",
            "\n",
            "Token stats:\n",
            "Files: 7\n",
            "Total tokens: 4630\n",
            "Example first 60 tokens: ['BAR', 'REST_6', 'POS_6', 'NOTE_74', 'DUR_6', 'REST_6', 'POS_6', 'NOTE_72', 'DUR_6', 'POS_0', 'NOTE_69', 'DUR_12', 'POS_0', 'NOTE_67', 'DUR_6', 'POS_6', 'NOTE_64', 'DUR_6', 'BAR', 'POS_0', 'NOTE_65', 'DUR_6', 'POS_6', 'NOTE_67', 'DUR_6', 'POS_0', 'NOTE_69', 'DUR_6', 'POS_6', 'NOTE_72', 'DUR_6', 'POS_0', 'NOTE_71', 'DUR_12', 'POS_0', 'NOTE_74', 'DUR_30', 'POS_6', 'NOTE_71', 'DUR_6', 'POS_0', 'NOTE_72', 'DUR_6', 'POS_6', 'NOTE_71', 'DUR_6', 'POS_0', 'NOTE_69', 'DUR_6', 'POS_6', 'NOTE_67', 'DUR_6', 'BAR', 'POS_0', 'NOTE_65', 'DUR_6', 'POS_6', 'NOTE_67', 'DUR_6', 'POS_0']\n",
            "\n",
            "Vocab size: 240\n",
            "\n",
            "Train ids: [3, 4, 2, 6, 0, 5]\n",
            "Val ids: [1]\n",
            "Train files: ['it_could_happen_to_you_miles_davis.mid', 'polkadots_and_moonbeams_chet_baker.mid', 'have_you_met_miss_jones_chet_baker.mid', 'there_will_never_be_another_you_woody_shaw.mid', 'autumn_leaves_chet_baker.mid', 'take_the_a_train_ryan_kisor.mid']\n",
            "Val files: ['autumn_leaves_wynton_marsalis.mid']\n",
            "Built 46 windows from 6 files. augment=True\n",
            "Built 2 windows from 1 files. augment=False\n",
            "\n",
            "Model on: cpu\n",
            "epoch 001 | train_loss=5.2669 | val_loss=4.1017\n",
            "  saved best checkpoint\n",
            "epoch 002 | train_loss=4.3265 | val_loss=3.6956\n",
            "  saved best checkpoint\n",
            "epoch 003 | train_loss=3.9625 | val_loss=3.6008\n",
            "  saved best checkpoint\n",
            "epoch 004 | train_loss=3.8082 | val_loss=3.3438\n",
            "  saved best checkpoint\n",
            "epoch 005 | train_loss=3.6810 | val_loss=3.1376\n",
            "  saved best checkpoint\n",
            "epoch 006 | train_loss=3.5201 | val_loss=3.0088\n",
            "  saved best checkpoint\n",
            "epoch 007 | train_loss=3.2665 | val_loss=2.8934\n",
            "  saved best checkpoint\n",
            "epoch 008 | train_loss=3.2321 | val_loss=2.8110\n",
            "  saved best checkpoint\n",
            "epoch 009 | train_loss=3.0700 | val_loss=2.6834\n",
            "  saved best checkpoint\n",
            "epoch 010 | train_loss=2.9192 | val_loss=2.6019\n",
            "  saved best checkpoint\n",
            "epoch 011 | train_loss=2.9017 | val_loss=2.5421\n",
            "  saved best checkpoint\n",
            "epoch 012 | train_loss=2.7348 | val_loss=2.4562\n",
            "  saved best checkpoint\n",
            "epoch 013 | train_loss=2.6805 | val_loss=2.4205\n",
            "  saved best checkpoint\n",
            "epoch 014 | train_loss=2.5498 | val_loss=2.3655\n",
            "  saved best checkpoint\n",
            "epoch 015 | train_loss=2.5208 | val_loss=2.3155\n",
            "  saved best checkpoint\n",
            "epoch 016 | train_loss=2.4504 | val_loss=2.2564\n",
            "  saved best checkpoint\n",
            "epoch 017 | train_loss=2.4621 | val_loss=2.2129\n",
            "  saved best checkpoint\n",
            "epoch 018 | train_loss=2.4422 | val_loss=2.2105\n",
            "  saved best checkpoint\n",
            "epoch 019 | train_loss=2.4138 | val_loss=2.2350\n",
            "epoch 020 | train_loss=2.3356 | val_loss=2.2469\n",
            "epoch 021 | train_loss=2.3312 | val_loss=2.2274\n",
            "epoch 022 | train_loss=2.2738 | val_loss=2.2084\n",
            "  saved best checkpoint\n",
            "epoch 023 | train_loss=2.2581 | val_loss=2.1876\n",
            "  saved best checkpoint\n",
            "epoch 024 | train_loss=2.2177 | val_loss=2.1716\n",
            "  saved best checkpoint\n",
            "epoch 025 | train_loss=2.2265 | val_loss=2.1686\n",
            "  saved best checkpoint\n",
            "epoch 026 | train_loss=2.1471 | val_loss=2.1686\n",
            "epoch 027 | train_loss=2.2559 | val_loss=2.1949\n",
            "epoch 028 | train_loss=2.1938 | val_loss=2.2041\n",
            "epoch 029 | train_loss=2.1731 | val_loss=2.2135\n",
            "epoch 030 | train_loss=2.1797 | val_loss=2.2070\n",
            "epoch 031 | train_loss=2.1273 | val_loss=2.1985\n",
            "epoch 032 | train_loss=2.0991 | val_loss=2.1419\n",
            "  saved best checkpoint\n",
            "epoch 033 | train_loss=2.1095 | val_loss=2.1579\n",
            "epoch 034 | train_loss=2.0982 | val_loss=2.1103\n",
            "  saved best checkpoint\n",
            "epoch 035 | train_loss=2.0007 | val_loss=2.1008\n",
            "  saved best checkpoint\n",
            "epoch 036 | train_loss=2.0514 | val_loss=2.1023\n",
            "epoch 037 | train_loss=2.0376 | val_loss=2.0281\n",
            "  saved best checkpoint\n",
            "epoch 038 | train_loss=2.0042 | val_loss=2.0198\n",
            "  saved best checkpoint\n",
            "epoch 039 | train_loss=1.9798 | val_loss=2.0132\n",
            "  saved best checkpoint\n",
            "epoch 040 | train_loss=1.9412 | val_loss=1.9802\n",
            "  saved best checkpoint\n",
            "epoch 041 | train_loss=1.9194 | val_loss=1.9332\n",
            "  saved best checkpoint\n",
            "epoch 042 | train_loss=1.8995 | val_loss=1.9298\n",
            "  saved best checkpoint\n",
            "epoch 043 | train_loss=1.8934 | val_loss=1.9289\n",
            "  saved best checkpoint\n",
            "epoch 044 | train_loss=1.8755 | val_loss=1.9099\n",
            "  saved best checkpoint\n",
            "epoch 045 | train_loss=1.8830 | val_loss=1.9119\n",
            "epoch 046 | train_loss=1.8825 | val_loss=1.9209\n",
            "epoch 047 | train_loss=1.8753 | val_loss=1.9261\n",
            "epoch 048 | train_loss=1.8578 | val_loss=1.8966\n",
            "  saved best checkpoint\n",
            "epoch 049 | train_loss=1.8114 | val_loss=1.8709\n",
            "  saved best checkpoint\n",
            "epoch 050 | train_loss=1.8128 | val_loss=1.8704\n",
            "  saved best checkpoint\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 0) Setup (install + imports + GPU check)\n",
        "# =========================================\n",
        "!pip -q install miditoolkit tqdm\n",
        "\n",
        "import os, glob, zipfile, random, math\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import miditoolkit\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available?\", torch.cuda.is_available())\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "# =========================================\n",
        "# 1) MIDI files -> midi_paths (Colab: upload ZIP; local: use data/raw)\n",
        "# =========================================\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    uploaded = files.upload()  # upload your zip of 10 MIDIs (no-metronome)\n",
        "    zip_name = next(iter(uploaded.keys()))\n",
        "    DATA_DIR = \"/content/midis\"\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_name, \"r\") as z:\n",
        "        z.extractall(DATA_DIR)\n",
        "    CKPT_DIR = \"/content/ckpt\"\n",
        "    OUTPUT_DIR = \"/content\"\n",
        "else:\n",
        "    _root = os.path.abspath(os.getcwd()) if os.path.isdir(\"data\") else os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "    DATA_DIR = os.path.join(_root, \"data\", \"raw\")\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    CKPT_DIR = os.path.join(_root, \"checkpoints\")\n",
        "    os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "    OUTPUT_DIR = os.path.join(_root, \"outputs\")\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "midi_paths = sorted(glob.glob(os.path.join(DATA_DIR, \"**/*.mid\"), recursive=True)) + \\\n",
        "            sorted(glob.glob(os.path.join(DATA_DIR, \"**/*.midi\"), recursive=True))\n",
        "\n",
        "print(\"Running in\", \"Colab\" if IN_COLAB else \"local\")\n",
        "print(\"Found MIDI files:\", len(midi_paths))\n",
        "print(\"Example paths:\", midi_paths[:3])\n",
        "\n",
        "# =========================================\n",
        "# 2) Quick scan: tracks/drums/polyphony stats\n",
        "# =========================================\n",
        "def max_distinct_pitches_same_start(m):\n",
        "    inst = m.instruments[0]\n",
        "    by = defaultdict(set)\n",
        "    for n in inst.notes:\n",
        "        by[n.start].add(n.pitch)\n",
        "    return max((len(v) for v in by.values()), default=0)\n",
        "\n",
        "def max_simultaneous_notes(notes):\n",
        "    events = []\n",
        "    for n in notes:\n",
        "        events.append((n.start, 1))\n",
        "        events.append((n.end, -1))\n",
        "    events.sort()\n",
        "    cur = 0\n",
        "    mx = 0\n",
        "    for _, d in events:\n",
        "        cur += d\n",
        "        mx = max(mx, cur)\n",
        "    return mx\n",
        "\n",
        "print(\"\\nname | tracks | drums | notes | same_start | max_poly\")\n",
        "for p in midi_paths:\n",
        "    m = miditoolkit.MidiFile(p)\n",
        "    inst = m.instruments[0]\n",
        "    print(f\"{os.path.basename(p)[:28]:28} | {len(m.instruments):6d} | {str(any(i.is_drum for i in m.instruments)):5s} \"\n",
        "          f\"| {len(inst.notes):5d} | {max_distinct_pitches_same_start(m):10d} | {max_simultaneous_notes(inst.notes):8d}\")\n",
        "\n",
        "# =========================================\n",
        "# 3) Clean to monophonic + tokenize (BAR + POS + REST + NOTE + DUR)\n",
        "# =========================================\n",
        "STEPS_PER_BEAT = 12\n",
        "BAR_STEPS = 4 * STEPS_PER_BEAT  # assume 4/4\n",
        "MAX_DUR = 48\n",
        "MAX_REST = 48\n",
        "\n",
        "def ticks_to_steps(ticks, tpb, spb=STEPS_PER_BEAT):\n",
        "    return int(round((ticks / tpb) * spb))\n",
        "\n",
        "def clean_to_monophonic(notes, tpb, spb=STEPS_PER_BEAT):\n",
        "    \"\"\"\n",
        "    Convert messy/polyphonic notes into a single line by:\n",
        "    1) quantize to steps\n",
        "    2) keep 1 note per onset step (longest dur, tie -> highest pitch)\n",
        "    3) trim overlaps\n",
        "    Returns list of (start_step, end_step, pitch)\n",
        "    \"\"\"\n",
        "    if not notes:\n",
        "        return []\n",
        "\n",
        "    q = []\n",
        "    for n in notes:\n",
        "        s = ticks_to_steps(n.start, tpb, spb)\n",
        "        e = ticks_to_steps(n.end,   tpb, spb)\n",
        "        if e <= s:\n",
        "            e = s + 1\n",
        "        q.append((s, e, int(n.pitch)))\n",
        "\n",
        "    by_s = defaultdict(list)\n",
        "    for s, e, p in q:\n",
        "        by_s[s].append((s, e, p))\n",
        "\n",
        "    kept = [max(g, key=lambda x: ((x[1]-x[0]), x[2])) for g in by_s.values()]\n",
        "    kept.sort(key=lambda x: (x[0], x[2]))\n",
        "\n",
        "    mono = []\n",
        "    for s, e, p in kept:\n",
        "        if mono and s < mono[-1][1]:\n",
        "            ps, pe, pp = mono[-1]\n",
        "            mono[-1] = (ps, s, pp)\n",
        "            if mono[-1][1] <= mono[-1][0]:\n",
        "                mono.pop()\n",
        "        mono.append((s, e, p))\n",
        "\n",
        "    mono = [x for x in mono if x[1] > x[0]]\n",
        "    return mono\n",
        "\n",
        "def tokens_from_mono_with_pos(mono, max_rest=MAX_REST, max_dur=MAX_DUR, add_bar=True):\n",
        "    def emit(out, prefix, v, cap):\n",
        "        while v > cap:\n",
        "            out.append(f\"{prefix}_{cap}\")\n",
        "            v -= cap\n",
        "        out.append(f\"{prefix}_{v}\")\n",
        "\n",
        "    toks = []\n",
        "    cur = 0\n",
        "    for s, e, p in mono:\n",
        "        # mark bar at note start if exactly on boundary\n",
        "        if add_bar and (s % BAR_STEPS == 0):\n",
        "            toks.append(\"BAR\")\n",
        "\n",
        "        # advance time with REST chunks (and insert BAR when crossing boundaries)\n",
        "        while cur < s:\n",
        "            if add_bar and (cur % BAR_STEPS == 0):\n",
        "                toks.append(\"BAR\")\n",
        "            step = min(s - cur, max_rest)\n",
        "            toks.append(f\"REST_{step}\")\n",
        "            cur += step\n",
        "\n",
        "        # beat-position anchor\n",
        "        toks.append(f\"POS_{s % STEPS_PER_BEAT}\")\n",
        "\n",
        "        toks.append(f\"NOTE_{p}\")\n",
        "        dur = max(1, e - s)\n",
        "        emit(toks, \"DUR\", dur, max_dur)\n",
        "        cur = e\n",
        "\n",
        "    return toks\n",
        "\n",
        "# Build per-file tokens\n",
        "per_file_tokens = []\n",
        "per_file_mono = []\n",
        "for p in midi_paths:\n",
        "    m = miditoolkit.MidiFile(p)\n",
        "    inst = m.instruments[0]\n",
        "    mono = clean_to_monophonic(inst.notes, m.ticks_per_beat, spb=STEPS_PER_BEAT)\n",
        "    toks = tokens_from_mono_with_pos(mono, add_bar=True)\n",
        "    per_file_mono.append(mono)\n",
        "    per_file_tokens.append(toks)\n",
        "\n",
        "print(\"\\nToken stats:\")\n",
        "print(\"Files:\", len(per_file_tokens))\n",
        "print(\"Total tokens:\", sum(len(t) for t in per_file_tokens))\n",
        "print(\"Example first 60 tokens:\", per_file_tokens[0][:60])\n",
        "\n",
        "# =========================================\n",
        "# 4) Build vocab + encode\n",
        "# =========================================\n",
        "PAD, BOS, EOS = \"<PAD>\", \"<BOS>\", \"<EOS>\"\n",
        "\n",
        "vocab = [PAD, BOS, EOS, \"BAR\"]\n",
        "vocab += [f\"POS_{i}\" for i in range(STEPS_PER_BEAT)]\n",
        "vocab += [f\"NOTE_{p}\" for p in range(128)]\n",
        "vocab += [f\"DUR_{d}\" for d in range(1, MAX_DUR+1)]\n",
        "vocab += [f\"REST_{r}\" for r in range(1, MAX_REST+1)]\n",
        "\n",
        "stoi = {t:i for i,t in enumerate(vocab)}\n",
        "itos = {i:t for t,i in stoi.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(\"\\nVocab size:\", vocab_size)\n",
        "\n",
        "def encode(tokens): return [stoi[t] for t in tokens]\n",
        "def decode(ids): return [itos[i] for i in ids]\n",
        "\n",
        "encoded_files = [encode(toks) for toks in per_file_tokens]\n",
        "\n",
        "# =========================================\n",
        "# 5) Train/Val split by FILE (no leakage)\n",
        "# =========================================\n",
        "rng = random.Random(42)\n",
        "idxs = list(range(len(encoded_files)))\n",
        "rng.shuffle(idxs)\n",
        "\n",
        "val_n = max(1, len(idxs)//5)  # ~20% val (2 files if you have 10)\n",
        "val_ids = sorted(idxs[:val_n])\n",
        "train_ids = [i for i in idxs if i not in set(val_ids)]\n",
        "\n",
        "print(\"\\nTrain ids:\", train_ids)\n",
        "print(\"Val ids:\", val_ids)\n",
        "print(\"Train files:\", [os.path.basename(midi_paths[i]) for i in train_ids])\n",
        "print(\"Val files:\", [os.path.basename(midi_paths[i]) for i in val_ids])\n",
        "\n",
        "# =========================================\n",
        "# 6) Window Dataset + (optional) transpose augmentation\n",
        "# =========================================\n",
        "block_size = 256\n",
        "stride = 64\n",
        "\n",
        "# Use observed pitch range if you want safer transposition\n",
        "NOTE_MIN = 39\n",
        "NOTE_MAX = 100\n",
        "\n",
        "NOTE0 = stoi[\"NOTE_0\"]\n",
        "NOTE127 = stoi[\"NOTE_127\"]\n",
        "\n",
        "def is_note_id(tid): return NOTE0 <= tid <= NOTE127\n",
        "def note_pitch_from_id(tid): return tid - NOTE0\n",
        "def note_id_from_pitch(p): return NOTE0 + p\n",
        "\n",
        "class JazzWindowDataset(Dataset):\n",
        "    def __init__(self, encoded_files, file_ids, block_size=256, stride=64, augment=False):\n",
        "        self.encoded_files = encoded_files\n",
        "        self.file_ids = file_ids\n",
        "        self.block_size = block_size\n",
        "        self.augment = augment\n",
        "        self.windows = []\n",
        "\n",
        "        for fid in file_ids:\n",
        "            seq = encoded_files[fid]\n",
        "            for s in range(0, len(seq) - block_size, stride):\n",
        "                self.windows.append((fid, s))\n",
        "\n",
        "        print(f\"Built {len(self.windows)} windows from {len(file_ids)} files. augment={augment}\")\n",
        "\n",
        "    def __len__(self): return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fid, s = self.windows[idx]\n",
        "        seq = self.encoded_files[fid]\n",
        "        chunk = seq[s:s+self.block_size]  # length block_size\n",
        "\n",
        "        # transpose augmentation: only affect NOTE tokens\n",
        "        if self.augment:\n",
        "            pitches = [note_pitch_from_id(t) for t in chunk if is_note_id(t)]\n",
        "            if pitches:\n",
        "                lo, hi = min(pitches), max(pitches)\n",
        "                down = NOTE_MIN - lo\n",
        "                up = NOTE_MAX - hi\n",
        "                if down <= up:\n",
        "                    shift = random.randint(down, up)\n",
        "                    if shift != 0:\n",
        "                        chunk = [\n",
        "                            (note_id_from_pitch(note_pitch_from_id(t) + shift) if is_note_id(t) else t)\n",
        "                            for t in chunk\n",
        "                        ]\n",
        "\n",
        "        x = [stoi[BOS]] + chunk          # len = block_size+1\n",
        "        y = chunk + [stoi[EOS]]          # len = block_size+1\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "train_ds = JazzWindowDataset(encoded_files, train_ids, block_size=block_size, stride=stride, augment=True)\n",
        "val_ds   = JazzWindowDataset(encoded_files, val_ids,   block_size=block_size, stride=stride, augment=False)\n",
        "\n",
        "batch_size = 64 if device == \"cuda\" else 16\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "# =========================================\n",
        "# 7) GPT-style Transformer (decoder-only via causal mask)\n",
        "# =========================================\n",
        "class GPTMini(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=6, dropout=0.15, max_len=block_size+2):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=4*d_model,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=False,  # avoids that nested-tensor warning sometimes\n",
        "            activation=\"gelu\",\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        if T > self.max_len:\n",
        "            raise ValueError(f\"T={T} > max_len={self.max_len}\")\n",
        "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        causal = torch.triu(torch.ones(T, T, device=idx.device), diagonal=1).bool()\n",
        "        x = self.encoder(x, mask=causal)\n",
        "\n",
        "        x = self.ln(x)\n",
        "        return self.head(x)  # (B,T,V)\n",
        "\n",
        "model = GPTMini(vocab_size).to(device)\n",
        "print(\"\\nModel on:\", next(model.parameters()).device)\n",
        "\n",
        "# =========================================\n",
        "# 8) Train loop + eval + checkpoint\n",
        "# =========================================\n",
        "def run_eval(model, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss(ignore_index=stoi[PAD])\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = ce(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            total_tokens += y.numel()\n",
        "    return total_loss / max(1, total_tokens)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "ce = nn.CrossEntropyLoss(ignore_index=stoi[PAD])\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "EPOCHS = 50\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running = []\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running.append(loss.item())\n",
        "\n",
        "    val_loss = run_eval(model, val_loader)\n",
        "    train_loss = float(np.mean(running)) if running else float(\"nan\")\n",
        "    print(f\"epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        torch.save({\n",
        "            \"model\": model.state_dict(),\n",
        "            \"opt\": opt.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"stoi\": stoi,\n",
        "            \"itos\": itos,\n",
        "            \"config\": {\"STEPS_PER_BEAT\": STEPS_PER_BEAT, \"block_size\": block_size}\n",
        "        }, os.path.join(CKPT_DIR, \"best.pt\"))\n",
        "        print(\"  saved best checkpoint\")\n",
        "\n",
        "# =========================================\n",
        "# 9) Compact generation (grammar masks) + MIDI writer\n",
        "# =========================================\n",
        "def build_type_masks(vocab, stoi, device):\n",
        "    def is_note(tok): return tok.startswith(\"NOTE_\")\n",
        "    def is_dur(tok):  return tok.startswith(\"DUR_\")\n",
        "    def is_rest(tok): return tok.startswith(\"REST_\")\n",
        "    def is_pos(tok):  return tok.startswith(\"POS_\")\n",
        "\n",
        "    banned = torch.zeros(len(vocab), dtype=torch.bool, device=device)\n",
        "    banned[stoi[PAD]] = True\n",
        "    banned[stoi[BOS]] = True\n",
        "\n",
        "    masks = {}\n",
        "    masks[\"NOTE\"] = torch.tensor([is_note(t) for t in vocab], device=device) & ~banned\n",
        "    masks[\"DUR\"]  = torch.tensor([is_dur(t)  for t in vocab], device=device) & ~banned\n",
        "    masks[\"REST\"] = torch.tensor([is_rest(t) for t in vocab], device=device) & ~banned\n",
        "    masks[\"POS\"]  = torch.tensor([is_pos(t)  for t in vocab], device=device) & ~banned\n",
        "\n",
        "    eos_mask = torch.zeros(len(vocab), dtype=torch.bool, device=device)\n",
        "    eos_mask[stoi[EOS]] = True\n",
        "    masks[\"EOS\"] = eos_mask & ~banned\n",
        "\n",
        "    bar_mask = torch.zeros(len(vocab), dtype=torch.bool, device=device)\n",
        "    bar_mask[stoi[\"BAR\"]] = True\n",
        "    masks[\"BAR\"] = bar_mask & ~banned\n",
        "\n",
        "    return masks\n",
        "\n",
        "masks = build_type_masks(vocab, stoi, device=device)\n",
        "\n",
        "def topk_sample(logits, k=20, temperature=0.9):\n",
        "    logits = logits / max(temperature, 1e-6)\n",
        "    if k is not None and k < logits.numel():\n",
        "        v, _ = torch.topk(logits, k)\n",
        "        logits = logits.masked_fill(logits < v[-1], -1e9)\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(probs, 1).item()\n",
        "\n",
        "def generate_tokens(model, max_new_tokens=900, temperature=0.9, top_k=20):\n",
        "    \"\"\"\n",
        "    Grammar (based on how we tokenized):\n",
        "      BOS -> BAR or POS\n",
        "      BAR -> REST or POS\n",
        "      REST -> REST or BAR or POS\n",
        "      POS -> NOTE\n",
        "      NOTE -> DUR\n",
        "      DUR -> REST or BAR or POS or EOS\n",
        "    \"\"\"\n",
        "    allowed_by_prev = {\n",
        "        \"BOS\":  (masks[\"BAR\"] | masks[\"POS\"]),\n",
        "        \"BAR\":  (masks[\"REST\"] | masks[\"POS\"]),\n",
        "        \"REST\": (masks[\"REST\"] | masks[\"BAR\"] | masks[\"POS\"]),\n",
        "        \"POS\":  masks[\"NOTE\"],\n",
        "        \"NOTE\": masks[\"DUR\"],\n",
        "        \"DUR\":  (masks[\"REST\"] | masks[\"BAR\"] | masks[\"POS\"] | masks[\"EOS\"]),\n",
        "    }\n",
        "\n",
        "    model.eval()\n",
        "    ids = [stoi[BOS]]\n",
        "    prev_type = \"BOS\"\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        x = torch.tensor(ids[-(block_size+1):], device=device).unsqueeze(0)\n",
        "        logits = model(x)[0, -1]  # (V,)\n",
        "\n",
        "        allowed = allowed_by_prev.get(prev_type, (masks[\"REST\"] | masks[\"POS\"] | masks[\"BAR\"] | masks[\"EOS\"]))\n",
        "        logits = logits.masked_fill(~allowed, -1e9)\n",
        "\n",
        "        nxt = topk_sample(logits, k=top_k, temperature=temperature)\n",
        "        ids.append(nxt)\n",
        "\n",
        "        tok = itos[nxt]\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        if tok == \"BAR\":\n",
        "            prev_type = \"BAR\"\n",
        "        else:\n",
        "            prev_type = tok.split(\"_\")[0]  # POS / NOTE / DUR / REST\n",
        "\n",
        "    return [itos[i] for i in ids]\n",
        "\n",
        "def tokens_to_midi(tokens, out_path=\"generated.mid\", tempo=140, steps_per_beat=STEPS_PER_BEAT):\n",
        "    tpb = 480\n",
        "    ticks_per_step = tpb // steps_per_beat\n",
        "\n",
        "    midi = miditoolkit.MidiFile(ticks_per_beat=tpb)\n",
        "    midi.tempo_changes = [miditoolkit.TempoChange(tempo, 0)]\n",
        "    inst = miditoolkit.Instrument(program=56, is_drum=False, name=\"Trumpet\")\n",
        "\n",
        "    t = 0\n",
        "    pending_pitch = None\n",
        "\n",
        "    for tok in tokens:\n",
        "        if tok in (PAD, BOS):\n",
        "            continue\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        if tok == \"BAR\" or tok.startswith(\"POS_\"):\n",
        "            continue\n",
        "\n",
        "        typ, val = tok.split(\"_\")\n",
        "        val = int(val)\n",
        "\n",
        "        if typ == \"REST\":\n",
        "            t += val\n",
        "            pending_pitch = None\n",
        "        elif typ == \"NOTE\":\n",
        "            pending_pitch = val\n",
        "        elif typ == \"DUR\" and pending_pitch is not None:\n",
        "            start = t * ticks_per_step\n",
        "            end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEEGq4BztkW5"
      },
      "source": [
        "#Load Best Checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRv7g32jsaMb",
        "outputId": "fc512322-e39d-4a79-c2a8-2e5208b5ca21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded best checkpoint from epoch: 50 val_loss: 1.870410680770874\n"
          ]
        }
      ],
      "source": [
        "ckpt = torch.load(os.path.join(CKPT_DIR, \"best.pt\"), map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"Loaded best checkpoint from epoch:\", ckpt[\"epoch\"], \"val_loss:\", ckpt[\"val_loss\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wWo9v86Ttpxx"
      },
      "outputs": [],
      "source": [
        "def tokens_to_midi(tokens, out_path=\"generated.mid\", tempo=140, steps_per_beat=STEPS_PER_BEAT):\n",
        "    tpb = 480\n",
        "    ticks_per_step = tpb // steps_per_beat\n",
        "\n",
        "    midi = miditoolkit.MidiFile(ticks_per_beat=tpb)\n",
        "    midi.tempo_changes = [miditoolkit.TempoChange(tempo, 0)]\n",
        "    inst = miditoolkit.Instrument(program=56, is_drum=False, name=\"Trumpet\")\n",
        "\n",
        "    t = 0\n",
        "    pending_pitch = None\n",
        "\n",
        "    for tok in tokens:\n",
        "        if tok in (PAD, BOS):\n",
        "            continue\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        if tok == \"BAR\" or tok.startswith(\"POS_\"):\n",
        "            continue\n",
        "\n",
        "        typ, val = tok.split(\"_\")\n",
        "        val = int(val)\n",
        "\n",
        "        if typ == \"REST\":\n",
        "            t += val\n",
        "            pending_pitch = None\n",
        "        elif typ == \"NOTE\":\n",
        "            pending_pitch = val\n",
        "        elif typ == \"DUR\" and pending_pitch is not None:\n",
        "            start = t * ticks_per_step\n",
        "            end = (t + val) * ticks_per_step\n",
        "            inst.notes.append(miditoolkit.Note(velocity=90, pitch=pending_pitch, start=start, end=end))\n",
        "            t += val\n",
        "            pending_pitch = None\n",
        "\n",
        "    midi.instruments.append(inst)\n",
        "    midi.dump(out_path)\n",
        "    return out_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKaQusrQt3ey",
        "outputId": "6dee7f37-d482-4a6c-d614-90986ff55d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrote gen_A_t0.75_k10.mid | bars: 5\n",
            "wrote gen_B_t0.85_k15.mid | bars: 11\n",
            "wrote gen_C_t0.95_k25.mid | bars: 12\n"
          ]
        }
      ],
      "source": [
        "settings = [\n",
        "    (\"A\", 0.75, 10),\n",
        "    (\"B\", 0.85, 15),\n",
        "    (\"C\", 0.95, 25),\n",
        "]\n",
        "\n",
        "for tag, temp, k in settings:\n",
        "    gen = generate_tokens(model, max_new_tokens=1400, temperature=temp, top_k=k)\n",
        "    path = os.path.join(OUTPUT_DIR, f\"gen_{tag}_t{temp}_k{k}.mid\")\n",
        "    tokens_to_midi(gen, out_path=path, tempo=140)\n",
        "    print(\"wrote\", path, \"| bars:\", sum(1 for t in gen if t==\"BAR\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9IFR0O8uDIr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
